{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wGs75ulgjxPo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.height = 5\n",
        "        self.width = 5\n",
        "        self.grid = np.zeros((self.height, self.width)) - 1  # Initialize grid with -1 rewards\n",
        "        self.current_location = (4, np.random.randint(0, self.width))  # Agent starts at a random column in the bottom row\n",
        "\n",
        "        self.bomb_location = (1, 3)\n",
        "        self.gold_location = (0, 3)\n",
        "        self.terminal_states = [self.bomb_location, self.gold_location]\n",
        "\n",
        "        # Set specific rewards for bomb and gold locations\n",
        "        self.grid[self.bomb_location[0], self.bomb_location[1]] = -10\n",
        "        self.grid[self.gold_location[0], self.gold_location[1]] = 10\n",
        "\n",
        "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "\n",
        "    def get_available_actions(self):\n",
        "        \"\"\"Returns possible actions\"\"\"\n",
        "        return self.actions\n",
        "\n",
        "    def agent_on_map(self):\n",
        "        \"\"\"Prints out current location of the agent on the grid (used for debugging)\"\"\"\n",
        "        grid = np.zeros((self.height, self.width))\n",
        "        grid[self.current_location[0], self.current_location[1]] = 1\n",
        "        return grid\n",
        "\n",
        "    def get_reward(self, location):\n",
        "        \"\"\"Returns the reward for an input position\"\"\"\n",
        "        return self.grid[location[0], location[1]]\n",
        "\n",
        "    def make_step(self, action):\n",
        "        \"\"\"\n",
        "        Moves the agent in specified direction.\n",
        "        If agent is at a border, agent stays still but takes a negative reward.\n",
        "        Returns the reward for the move.\n",
        "        \"\"\"\n",
        "        last_location = self.current_location\n",
        "\n",
        "        if action == 'UP':\n",
        "            if last_location[0] == 0:  # At the top border\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = (self.current_location[0] - 1, self.current_location[1])\n",
        "                reward = self.get_reward(self.current_location)\n",
        "\n",
        "        elif action == 'DOWN':\n",
        "            if last_location[0] == self.height - 1:  # At the bottom border\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = (self.current_location[0] + 1, self.current_location[1])\n",
        "                reward = self.get_reward(self.current_location)\n",
        "\n",
        "        elif action == 'LEFT':\n",
        "            if last_location[1] == 0:  # At the left border\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = (self.current_location[0], self.current_location[1] - 1)\n",
        "                reward = self.get_reward(self.current_location)\n",
        "\n",
        "        elif action == 'RIGHT':\n",
        "            if last_location[1] == self.width - 1:  # At the right border\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = (self.current_location[0], self.current_location[1] + 1)\n",
        "                reward = self.get_reward(self.current_location)\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def check_state(self):\n",
        "        \"\"\"\n",
        "        Check if the agent is in a terminal state (gold or bomb).\n",
        "        If so, return 'TERMINAL'.\n",
        "        \"\"\"\n",
        "        if self.current_location in self.terminal_states:\n",
        "            return 'TERMINAL'\n",
        "        return 'ONGOING'\n"
      ],
      "metadata": {
        "id": "4dla2fHPkCb-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomAgent():\n",
        "\n",
        "  def choose_action(self,get_available_actions):\n",
        "    \"\"\"Returns a random choice of the available actions\"\"\"\n",
        "    return np.random.choice(available_actions)"
      ],
      "metadata": {
        "id": "hoa2RY1zRTyz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Q_Agent():\n",
        "  def __init__(self,environment, episilon =0.05,alpha=0.1, gamma =1):\n",
        "    self.environment=environment\n",
        "    self.q_table = dict()\n",
        "    for x in range(environment.height):\n",
        "      for y in range(environment.width):\n",
        "        self.q_table[(x,y)]={'UP':0,'DOWN':0,'LEFT':0,'RIGHT':0}\n",
        "\n",
        "    self.episilon=episilon\n",
        "    self.alpha=alpha\n",
        "    self.gamma=gamma\n",
        "\n",
        "  def choose_action(self,available_actions):\n",
        "    \"\"\"Returs the optimal action from q-value table. If multiple optimal actions, chooses random choice. Will make an explotory randim action dependent on episilon\"\"\"\n",
        "\n",
        "    if np.random.uniform(0,1)<self.episilon:\n",
        "      action = available_actions[np.random.randint(0,len(available_actions))]\n",
        "\n",
        "    else:\n",
        "      q_values_of_state = self.q_table[self.environment.current_location]\n",
        "      maxValues = max(q_values_of_state.values())\n",
        "      action = np.random.choice([k for k, v in q_values_of_state.items() if v == maxValue])\n",
        "\n",
        "      return action\n",
        "\n",
        "  def learn(self,old_state, reward, new_state, action):\n",
        "    \"\"\"Update the Q- Value table using Q-Learning\"\"\"\n",
        "\n",
        "    q_values_of_state = self.q_table[new_state]\n",
        "    max_q_value_in_new_state = max(q_values_of_state.values())\n",
        "    current_q_value = self.q_table[old_state][action]\n",
        "\n",
        "    self.q_table[old_state][action]=(1-self.alpha)*current_q_value+self.alpha*(reward + self.gamma*max_q_value_in_new_state)\n",
        ""
      ],
      "metadata": {
        "id": "veEZ-AqgSED2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play(environment,agent,trials=500, max_steps_per_episode=1000,learn=False):\n",
        "  \"\"\"The play function runs iterations and updates Q-values if desired.\"\"\"\n",
        "  reward_per_episode =[]\n",
        "\n",
        "  for trial in range(trials):\n",
        "    cumulative_reward = 1\n",
        "    step = 0\n",
        "    gamma_over = False\n",
        "    while step < max_steps_per_episode and gamma_over != True:\n",
        "      old_state = environment.current_location\n",
        "      action = agent.choose_action(environment.actions)\n",
        "      reward=environment.make_step(action)\n",
        "      new_state = environment.current_location\n",
        "\n",
        "      if learn == True:\n",
        "        agent.learn(old_state,reward,new_state,action)\n",
        "\n",
        "        cumulative_reward += reward\n",
        "        step +=1\n",
        "\n",
        "      if environment.check_state() == 'TERMINAL':\n",
        "        environment.__init__()\n",
        "        game_over = True\n",
        "\n",
        "    reward_per_episode.append(cumulative_reward)\n",
        "\n",
        "    return reward_per_episode\n"
      ],
      "metadata": {
        "id": "qdu-PF2qSgBQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridWorld()\n",
        "agent = RandomAgent()\n",
        "\n",
        "print(\"Current position of thr agent =\", env.current_location)\n",
        "print(env.agent_on_map())\n",
        "available_actions=env.get_available_actions()\n",
        "print(\"Available Actions =\",available_actions)\n",
        "choosen_action =agent.choose_action(available_actions)\n",
        "print(\"Randomly chosen action = \",choosen_action)\n",
        "reward = env.make_step(choosen_action)\n",
        "\n",
        "print(\"Rward Obtained :\",reward)\n",
        "print(\"Current postion of the agent =\",env.current_location)\n",
        "print(env.agent_on_map())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLFDn4ilW8ty",
        "outputId": "fccff680-98aa-41cb-ee0b-eb98fbabad4d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current position of thr agent = (4, 2)\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]]\n",
            "Available Actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
            "Randomly chosen action =  LEFT\n",
            "Rward Obtained : -1.0\n",
            "Current postion of the agent = (4, 1)\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "environment = GridWorld()\n",
        "agentQ = RandomAgent()\n",
        "\n",
        "reward_per_episode=play(environment,agentQ, trials =500)\n",
        "plt.plot(reward_per_episode)"
      ],
      "metadata": {
        "id": "1makAKbuX7xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty(d,indent=0):\n",
        "  for key, value in d.items():\n",
        "    print('\\t'*indent + str(key))\n",
        "    if isinstance(value,dict):\n",
        "      pretty(value,indent+1)\n",
        "    else:\n",
        "      print('\\t'*(indent+1)+str(value))\n",
        "pretty(agentQ.q_table)"
      ],
      "metadata": {
        "id": "9QoOoofEYVlR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}